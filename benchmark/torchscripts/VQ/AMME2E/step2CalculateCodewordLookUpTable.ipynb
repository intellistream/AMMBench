{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have done step1SaveMtxToPt.cpp, you should get\n",
    "\n",
    "heyuhao@sutd:~/AMMBench$ ls /home/heyuhao/AMMBench/build/benchmark/torchscripts/VQ/AMME2E/MtxPt\n",
    "\n",
    "AST_A.pt  AST_B.pt  BUS_A.pt  BUS_B.pt  DWAVE_A.pt  DWAVE_B.pt  ECO_A.pt  ECO_B.pt  QCD_A.pt  QCD_B.pt  RDB_A.pt  RDB_B.pt  UTM_A.pt  UTM_B.pt  ZENIOS_A.pt  ZENIOS_B.pt\n",
    "\n",
    "This jupuyer is to generate the codeword & lookup table for each pair of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCodewordAndLookUpTable(A, B, m):\n",
    "    \n",
    "    # Sample matrix sizes and PQ parameters\n",
    "    A_rows, A_cols = A.shape\n",
    "    lA = A_rows // m // 10  # Number of centroids for each subspace\n",
    "    CA = A_cols // m  # Dimension of each subspace\n",
    "\n",
    "    # Sample matrix sizes and PQ parameters\n",
    "    B_rows, B_cols = B.shape\n",
    "    lB = B_cols // m // 10  # Number of centroids for each subspace\n",
    "    CB = B_rows // m  # Dimension of each subspace\n",
    "\n",
    "    # Initialize lists to store subspaces and centroids\n",
    "    subspaces_A = []\n",
    "    codewords_A = []\n",
    "\n",
    "    # Create subspaces and centroids\n",
    "    for i in range(m):\n",
    "        subspace_A = A[:, i * CA : (i + 1) * CA]  # 500*20\n",
    "        subspaces_A.append(subspace_A)\n",
    "\n",
    "        # Apply KMeans on the row vectors within the subspace\n",
    "        kmeans = KMeans(n_clusters=lA, n_init=10)\n",
    "        kmeans.fit(subspace_A)  # 500*20\n",
    "        subspace_centroids_A = torch.tensor(kmeans.cluster_centers_) # 10*20\n",
    "        codewords_A.append(subspace_centroids_A)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    subspaces_A = torch.stack(subspaces_A, dim=0)\n",
    "    codewords_A = torch.stack(codewords_A, dim=0) # 5*10*20\n",
    "\n",
    "    print(\"Subspaces A shape:\", subspaces_A.shape) # torch.Size([5, 500, 20])\n",
    "    print(\"Codewords A shape:\", codewords_A.shape) # torch.Size([5, 10, 20])\n",
    "\n",
    "\n",
    "    # Initialize lists to store subspaces and centroids\n",
    "    subspaces_B = []\n",
    "    codewords_B = []\n",
    "\n",
    "    # Create subspaces and centroids\n",
    "    for k in range(m):\n",
    "        subspace_B = B[k * CB : (k + 1) * CB, :]  # Extract the subspace along x-axis\n",
    "        subspaces_B.append(subspace_B)\n",
    "\n",
    "        # Apply KMeans on the column vectors within the subspace\n",
    "        kmeans = KMeans(n_clusters=lB, n_init=10)\n",
    "        kmeans.fit(subspace_B.T)  # Transpose to cluster along columns (column vectors)\n",
    "        subspace_centroids_B = torch.tensor(kmeans.cluster_centers_)\n",
    "        codewords_B.append(subspace_centroids_B)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    subspaces_B = torch.stack(subspaces_B, dim=0)\n",
    "    codewords_B = torch.stack(codewords_B, dim=0)\n",
    "\n",
    "    print(\"Subspaces B shape:\", subspaces_B.shape)  # torch.Size([5, 20, 300])\n",
    "    print(\"Codewords B shape:\", codewords_B.shape)  # torch.Size([5, 6, 20])\n",
    "\n",
    "    # Sample precomputed codewords for A and B (You should replace these with your actual codewords)\n",
    "    lookup_table = torch.zeros((m,lA,lB))\n",
    "    for i in range(m):\n",
    "        for j in range(lA):\n",
    "            for k in range(lB):\n",
    "                lookup_table[i][j][k] = torch.matmul(codewords_A[i][j], codewords_B[i][k]) # for each subspace, get catersian product of A,B codeword\n",
    "\n",
    "    print(\"lookup_table.shape: \", lookup_table.shape)\n",
    "\n",
    "    return lA, lB, codewords_A, codewords_B, lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to save codeword and lookup_table to pt\n",
    "class TensorContainer(nn.Module):\n",
    "    def __init__(self, tensor_dict):\n",
    "        super().__init__()\n",
    "        for key,value in tensor_dict.items():\n",
    "            setattr(self, key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(C, D, codewords_A, codewords_B, lookup_table):\n",
    "    m = codewords_A.shape[0]\n",
    "\n",
    "    # Sample matrix sizes and PQ parameters\n",
    "    C_rows, C_cols = C.shape\n",
    "    CC = codewords_A.shape[2]  # Dimension of each subspace for matrix C\n",
    "\n",
    "    D_rows, D_cols = D.shape\n",
    "    CD = codewords_B.shape[2]  # Dimension of each subspace for matrix D\n",
    "\n",
    "    # Initialize lists to store quantized indices\n",
    "    C_quantized = []\n",
    "    D_quantized = []\n",
    "\n",
    "    # Find the nearest codeword indices for matrix C\n",
    "    for i in range(m):\n",
    "        codewords_c = codewords_A[i] # torch.Size([10, 20])\n",
    "        C_subspace = C[:, i * CC : (i + 1) * CC] # torch.Size([500, 20])\n",
    "        distances = torch.norm(codewords_c.unsqueeze(0) - C_subspace.unsqueeze(1), dim=2, p=2) # torch.Size([500, 10]) = torch.Size([1, 10, 20]) - torch.Size([500, 1, 20])\n",
    "        closest_codeword_indices = torch.argmin(distances, dim=1) # torch.Size([500])\n",
    "        C_quantized.append(closest_codeword_indices)\n",
    "    C_quantized = torch.stack(C_quantized, dim=1) # torch.Size([500, 5])\n",
    "    # print(\"C quantized shape:\", C_quantized.shape)  # Shape of the quantized indices for C\n",
    "\n",
    "    # Find the nearest codeword indices for matrix D\n",
    "    for k in range(m):\n",
    "        codewords_d = codewords_B[k]\n",
    "        D_subspace = D[k * CD : (k + 1) * CD, :] # torch.Size([20, 300])\n",
    "        distances = torch.norm(codewords_d.unsqueeze(0) - torch.swapaxes(D_subspace.unsqueeze(1), 0, 2), dim=2, p=2) # torch.Size([300, 6]) = torch.Size([1, 6, 20]) - torch.Size([300, 1, 20])\n",
    "        closest_codeword_indices = torch.argmin(distances, dim=1) # torch.Size([300])\n",
    "        D_quantized.append(closest_codeword_indices.T)\n",
    "    D_quantized = torch.stack(D_quantized, dim=0) # torch.Size([5, 300])\n",
    "    # print(\"D quantized shape:\", D_quantized.shape)  # Shape of the quantized indices for D\n",
    "\n",
    "    # Define the batch size for batch processing\n",
    "    batch_size_C = C_rows\n",
    "    batch_size_D = D_cols\n",
    "\n",
    "    # Initialize the matrix products\n",
    "    matrix_products = torch.zeros((C_rows, D_cols))\n",
    "\n",
    "    # Perform matrix multiplication using batch processing\n",
    "    for i in range(0, C_rows, batch_size_C):\n",
    "        for j in range(0, D_cols, batch_size_D):\n",
    "            batch_result = torch.zeros((batch_size_C, batch_size_D))\n",
    "            \n",
    "            for k in range(m):\n",
    "                # Gather quantized indices for the current batch\n",
    "                C_indices = C_quantized[i:i+batch_size_C, k]\n",
    "                D_indices = D_quantized[k, j:j+batch_size_D]\n",
    "                \n",
    "                # Gather relevant entries from the lookup table\n",
    "                batch_lookup = lookup_table[k, C_indices, :][:, D_indices]\n",
    "                \n",
    "                # Accumulate the batch result\n",
    "                batch_result += batch_lookup\n",
    "            \n",
    "            # Assign the batch result to the corresponding position in the matrix products\n",
    "            matrix_products[i:i+batch_size_C, j:j+batch_size_D] = batch_result\n",
    "\n",
    "    E = torch.matmul(C, D)\n",
    "    return torch.norm(matrix_products-E)/torch.norm(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subspaces A shape: torch.Size([1, 765, 765])\n",
      "Codewords A shape: torch.Size([1, 76, 765])\n",
      "Subspaces B shape: torch.Size([1, 765, 765])\n",
      "Codewords B shape: torch.Size([1, 76, 765])\n",
      "lookup_table.shape:  torch.Size([1, 76, 76])\n",
      "AST 1 76 76 tensor(1.2885e-07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subspaces A shape: torch.Size([10, 765, 76])\n",
      "Codewords A shape: torch.Size([10, 7, 76])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subspaces B shape: torch.Size([10, 76, 765])\n",
      "Codewords B shape: torch.Size([10, 7, 76])\n",
      "lookup_table.shape:  torch.Size([10, 7, 7])\n",
      "AST 10 7 7 tensor(0.0065)\n",
      "Subspaces A shape: torch.Size([1, 4929, 10595])\n",
      "Codewords A shape: torch.Size([1, 492, 10595])\n",
      "Subspaces B shape: torch.Size([1, 10595, 4929])\n",
      "Codewords B shape: torch.Size([1, 492, 10595])\n",
      "lookup_table.shape:  torch.Size([1, 492, 492])\n",
      "BUS 1 492 492 tensor(5.7282e-06)\n",
      "Subspaces A shape: torch.Size([10, 4929, 1059])\n",
      "Codewords A shape: torch.Size([10, 49, 1059])\n",
      "Subspaces B shape: torch.Size([10, 1059, 4929])\n",
      "Codewords B shape: torch.Size([10, 49, 1059])\n",
      "lookup_table.shape:  torch.Size([10, 49, 49])\n",
      "BUS 10 49 49 tensor(9.7871e-05)\n",
      "Subspaces A shape: torch.Size([1, 512, 512])\n",
      "Codewords A shape: torch.Size([1, 51, 512])\n",
      "Subspaces B shape: torch.Size([1, 512, 512])\n",
      "Codewords B shape: torch.Size([1, 51, 512])\n",
      "lookup_table.shape:  torch.Size([1, 51, 51])\n",
      "DWAVE 1 51 51 tensor(0.0019)\n",
      "Subspaces A shape: torch.Size([10, 512, 51])\n",
      "Codewords A shape: torch.Size([10, 5, 51])\n",
      "Subspaces B shape: torch.Size([10, 51, 512])\n",
      "Codewords B shape: torch.Size([10, 5, 51])\n",
      "lookup_table.shape:  torch.Size([10, 5, 5])\n",
      "DWAVE 10 5 5 tensor(0.0044)\n",
      "Subspaces A shape: torch.Size([1, 207, 260])\n",
      "Codewords A shape: torch.Size([1, 20, 260])\n",
      "Subspaces B shape: torch.Size([1, 260, 207])\n",
      "Codewords B shape: torch.Size([1, 20, 260])\n",
      "lookup_table.shape:  torch.Size([1, 20, 20])\n",
      "ECO 1 20 20 tensor(0.2780)\n",
      "Subspaces A shape: torch.Size([10, 207, 26])\n",
      "Codewords A shape: torch.Size([10, 2, 26])\n",
      "Subspaces B shape: torch.Size([10, 26, 207])\n",
      "Codewords B shape: torch.Size([10, 2, 26])\n",
      "lookup_table.shape:  torch.Size([10, 2, 2])\n",
      "ECO 10 2 2 tensor(0.5436)\n",
      "Subspaces A shape: torch.Size([1, 3072, 3072])\n",
      "Codewords A shape: torch.Size([1, 307, 3072])\n",
      "Subspaces B shape: torch.Size([1, 3072, 3072])\n",
      "Codewords B shape: torch.Size([1, 307, 3072])\n",
      "lookup_table.shape:  torch.Size([1, 307, 307])\n",
      "QCD 1 307 307 tensor(0.0005)\n",
      "Subspaces A shape: torch.Size([10, 3072, 307])\n",
      "Codewords A shape: torch.Size([10, 30, 307])\n",
      "Subspaces B shape: torch.Size([10, 307, 3072])\n",
      "Codewords B shape: torch.Size([10, 30, 307])\n",
      "lookup_table.shape:  torch.Size([10, 30, 30])\n",
      "QCD 10 30 30 tensor(0.0016)\n",
      "Subspaces A shape: torch.Size([1, 2048, 2048])\n",
      "Codewords A shape: torch.Size([1, 204, 2048])\n",
      "Subspaces B shape: torch.Size([1, 2048, 2048])\n",
      "Codewords B shape: torch.Size([1, 204, 2048])\n",
      "lookup_table.shape:  torch.Size([1, 204, 204])\n",
      "RDB 1 204 204 tensor(0.0003)\n",
      "Subspaces A shape: torch.Size([10, 2048, 204])\n",
      "Codewords A shape: torch.Size([10, 20, 204])\n",
      "Subspaces B shape: torch.Size([10, 204, 2048])\n",
      "Codewords B shape: torch.Size([10, 20, 204])\n",
      "lookup_table.shape:  torch.Size([10, 20, 20])\n",
      "RDB 10 20 20 tensor(0.0039)\n",
      "Subspaces A shape: torch.Size([1, 1700, 1700])\n",
      "Codewords A shape: torch.Size([1, 170, 1700])\n",
      "Subspaces B shape: torch.Size([1, 1700, 1700])\n",
      "Codewords B shape: torch.Size([1, 170, 1700])\n",
      "lookup_table.shape:  torch.Size([1, 170, 170])\n",
      "UTM 1 170 170 tensor(0.9210)\n",
      "Subspaces A shape: torch.Size([10, 1700, 170])\n",
      "Codewords A shape: torch.Size([10, 17, 170])\n",
      "Subspaces B shape: torch.Size([10, 170, 1700])\n",
      "Codewords B shape: torch.Size([10, 17, 170])\n",
      "lookup_table.shape:  torch.Size([10, 17, 17])\n",
      "UTM 10 17 17 tensor(0.9502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (220) found smaller than n_clusters (287). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subspaces A shape: torch.Size([1, 2873, 2873])\n",
      "Codewords A shape: torch.Size([1, 287, 2873])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (220) found smaller than n_clusters (287). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subspaces B shape: torch.Size([1, 2873, 2873])\n",
      "Codewords B shape: torch.Size([1, 287, 2873])\n",
      "lookup_table.shape:  torch.Size([1, 287, 287])\n",
      "ZENIOS 1 287 287 tensor(1.2641e-08)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subspaces A shape: torch.Size([10, 2873, 287])\n",
      "Codewords A shape: torch.Size([10, 28, 287])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/heyuhao/.local/lib/python3.10/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subspaces B shape: torch.Size([10, 287, 2873])\n",
      "Codewords B shape: torch.Size([10, 28, 287])\n",
      "lookup_table.shape:  torch.Size([10, 28, 28])\n",
      "ZENIOS 10 28 28 tensor(0.0010)\n"
     ]
    }
   ],
   "source": [
    "datasetDir = '/home/heyuhao/AMMBench/build/benchmark/torchscripts/VQ/AMME2E/MtxPt'\n",
    "saveDir = '/home/heyuhao/AMMBench/benchmark/torchscripts/VQ/AMME2E/CodewordLookUpTable'\n",
    "\n",
    "for datasetName in [\"AST\",\"BUS\",\"DWAVE\",\"ECO\",\"QCD\",\"RDB\",\"UTM\",\"ZENIOS\"]:\n",
    "    for m in [1, 10]: # Number of subspaces\n",
    "\n",
    "        # load\n",
    "        A = torch.load(f'{datasetDir}/{datasetName}_A.pt')\n",
    "        B = torch.load(f'{datasetDir}/{datasetName}_B.pt')\n",
    "\n",
    "        # calculate codeword and lookup_table\n",
    "        lA, lB, codewords_A, codewords_B, lookup_table = getCodewordAndLookUpTable(A, B, m)\n",
    "\n",
    "        # calculate error\n",
    "        C=A # actually C,D are testing matrices, and usually different from training matrices A,B. but its ok to make them the same also, cuz we more focus on latency\n",
    "        D=B\n",
    "        relativeFroError = quantize(C, D, codewords_A, codewords_B, lookup_table)\n",
    "\n",
    "        # save codeword and lookup_table\n",
    "        tensor_dict = {\n",
    "            # 'A': A,\n",
    "            # 'B': B,\n",
    "            'codewordsA': codewords_A,\n",
    "            'codewordsB': codewords_B,\n",
    "            'lookUpTable': lookup_table,\n",
    "            'datasetName': datasetName,\n",
    "            'm': m,\n",
    "            'lA': lA,\n",
    "            'lB': lB,\n",
    "            'relativeFroError': relativeFroError\n",
    "        }\n",
    "        tensors = TensorContainer(tensor_dict)\n",
    "        tensors = torch.jit.script(tensors)\n",
    "        tensors.save(f'{saveDir}/{datasetName}_m{m}_lA{lA}_lB{lB}.pth')\n",
    "        print(datasetName, m, lA, lB, relativeFroError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matrix_products' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Assuming matrix_products is your 2D tensor\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m max_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(matrix_products)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m      5\u001b[0m min_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(matrix_products)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m      6\u001b[0m mean_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(matrix_products)\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matrix_products' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming matrix_products is your 2D tensor\n",
    "max_value = torch.max(matrix_products).item()\n",
    "min_value = torch.min(matrix_products).item()\n",
    "mean_value = torch.mean(matrix_products).item()\n",
    "\n",
    "# Convert tensor to a flattened 1D tensor\n",
    "flattened_matrix = matrix_products.view(-1)\n",
    "\n",
    "# Convert the tensor to a NumPy array for computing percentiles\n",
    "np_matrix = flattened_matrix.numpy()\n",
    "\n",
    "percentile_25 = np.percentile(np_matrix, 25)\n",
    "percentile_75 = np.percentile(np_matrix, 75)\n",
    "\n",
    "print(\"Maximum:\", max_value)\n",
    "print(\"Minimum:\", min_value)\n",
    "print(\"Mean:\", mean_value)\n",
    "print(\"25th Percentile:\", percentile_25)\n",
    "print(\"75th Percentile:\", percentile_75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
